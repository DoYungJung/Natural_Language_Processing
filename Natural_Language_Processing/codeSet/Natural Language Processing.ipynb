{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ca6d794-4f72-4cbb-990b-0a0771f909b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "912db7c5-acb1-48a0-a928-ec7fd4a89442",
   "metadata": {},
   "outputs": [],
   "source": [
    "정규표현식문법 = pd.read_excel(\"../dataSet/정규표현식문법.xlsx\", index_col= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c808747-6a51-4f2b-8008-73eddd7a6e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "역슬래시문자규칙 = pd.read_excel(\"../dataSet/역슬래시문자규칙.xlsx\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f793079-343b-4fcb-ab81-aad614379ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PoS태그리스트 = pd.read_excel(\"../dataSet/PoS태그리스트.xlsx\", index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c050d43-f4f6-4cc6-b504-b2aabca545de",
   "metadata": {},
   "source": [
    "## 자연어 처리(Natural Language Processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9fbc73-7876-4d7c-bdc1-4a9259eafc62",
   "metadata": {
    "tags": []
   },
   "source": [
    "###### 자연어는 일상 생활에서 사용하는 언어\n",
    "###### 자연어 처리는 자연어의 의미를 분석 처리하는 일\n",
    "###### 텍스트 분류, 감정분석, 문서요약, 번역, 질의 응답, 음성 임식, 챗봇과 같은 응용\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5922e7e-5e6d-4d9d-bcbe-85797df4b301",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. 텍스트 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f9f423d-49f3-4610-a0f0-fb85d1549bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'No pain no gain'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1207f1f8-0153-42be-ad2b-8066cffb48af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'pain' in s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15737d87-ec61-4409-9166-6c67a16de85d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['No', 'pain', 'no', 'gain']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "347930e1-71e8-4c6f-888d-fda1282fff81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.split().index('gain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d41b4132-576f-4cdc-bcd2-f0f0893863e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gain'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[-4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "920de36f-721a-4f05-b3ff-5050c0e744d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pain'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.split()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b8cb27c-1cc4-438b-a40b-792801029ea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'on'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.split()[2][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b89e7db2-1e66-4b87-bf32-96a2cdcdf9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "s =\"한글도 처리 가능\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b6df6f0-9ac4-49af-9ad1-8089f412bd7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"처리\" in s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bbda5e0-420b-42a1-9ef4-555eb632e6fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['한글도', '처리', '가능']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0596d2c2-397f-4f0f-b63a-5dbecf6dc918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'한글도'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.split()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7679e0-e5f9-41a1-bc1c-683a4fc90c72",
   "metadata": {},
   "source": [
    "### 2. 영어 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d74d671-89df-4285-a7c1-f061047c3a01",
   "metadata": {},
   "source": [
    "#### 대소문자 통합"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d592b5-85a7-4e9e-88f2-ef14dda59071",
   "metadata": {},
   "source": [
    "###### 대소문자를 통합하지 않는다면 컴퓨터는 같은 단어를 다르게 받아들임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d960877-7a52-42d7-a31e-9fa9fe9320ca",
   "metadata": {},
   "source": [
    "###### 파이썬 내장함수 lower(), upper()를 통해 간단하게 통합 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ceb05ecf-1a66-4c82-9737-9ff2c92d954b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abcdefgh ABCDEFGH\n"
     ]
    }
   ],
   "source": [
    "s = 'AbCdEfgh'\n",
    "str_lower = s.lower()\n",
    "str_upper = s.upper()\n",
    "print(str_lower,str_upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c203b01c-a0a4-4bcc-9357-2985adf7bd92",
   "metadata": {},
   "source": [
    "### 2-1 정규화(Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3da6b928-8d93-43ee-ab52-5331217b4701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I visited UK from US on 22-09-20\n"
     ]
    }
   ],
   "source": [
    "s = \"I visited UK from US on 22-09-20\"\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2327f75a-b778-4535-bf07-19f714965d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I visited United Kingdom from United States on 22-09-2020\n"
     ]
    }
   ],
   "source": [
    "new_s = s.replace(\"UK\", \"United Kingdom\").replace(\"US\", \"United States\").replace(\"-20\",\"-2020\")\n",
    "print(new_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a83a35-9d28-4f15-a495-4ee5a0c5fe71",
   "metadata": {},
   "source": [
    "### 2-2 정규표현식(Regular Expression)\n",
    "\n",
    "###### 정규표현식은 특정 문자들을 편리하게 지정하고 추가, 삭제 가능\n",
    "###### 데이터 전처리에서 정규표현식을 많이 사용\n",
    "###### 파이썬에서는 정규 표현식을 지원하는 re패키지 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1984390-341e-4ed5-8f4d-d8063aa34c4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>설명</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>특수문자</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>앞의 문자 1개를 표현</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>?</th>\n",
       "      <td>문자 한 개를 표현하거나 존재할 수도, 존재하지 않을 수도 있음(0개 또는 1개)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>*</th>\n",
       "      <td>앞의 문자가 0개 이상</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>+</th>\n",
       "      <td>앞의 문자가 최소 1개 이상</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>^</th>\n",
       "      <td>뒤의 문자로 문자열이 시작</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>＼$</th>\n",
       "      <td>앞의 문자로 문자열이 끝남</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>＼{n＼}</th>\n",
       "      <td>n 번만큼 반복</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>＼{n1, n2＼}</th>\n",
       "      <td>n1 이상, n2 이하만큼 반복, n2를 지정하지 않으면 n1이상만 반복</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>＼[ abc ＼]</th>\n",
       "      <td>안에 문자들 중에 한 개의 문자와 매치, a-z처럼 범위도 지정 가능</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>＼[ ^a ＼]</th>\n",
       "      <td>해당 문자를 제외하고 매치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a:b</th>\n",
       "      <td>a 또는 b를 나타냄</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       설명\n",
       "특수문자                                                     \n",
       ".                                            앞의 문자 1개를 표현\n",
       "?           문자 한 개를 표현하거나 존재할 수도, 존재하지 않을 수도 있음(0개 또는 1개)\n",
       "*                                            앞의 문자가 0개 이상\n",
       "+                                         앞의 문자가 최소 1개 이상\n",
       "^                                          뒤의 문자로 문자열이 시작\n",
       "＼$                                         앞의 문자로 문자열이 끝남\n",
       "＼{n＼}                                            n 번만큼 반복\n",
       "＼{n1, n2＼}       n1 이상, n2 이하만큼 반복, n2를 지정하지 않으면 n1이상만 반복\n",
       "＼[ abc ＼]          안에 문자들 중에 한 개의 문자와 매치, a-z처럼 범위도 지정 가능\n",
       "＼[ ^a ＼]                                   해당 문자를 제외하고 매치\n",
       "a:b                                           a 또는 b를 나타냄"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "정규표현식문법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35e2e41-8dd3-4fbe-a103-b6614bebee5c",
   "metadata": {},
   "source": [
    "###### 정규 표현식에 자주 사용하는 역슬래시(＼)를 이용한 문자 규칙"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adfb5376-d6ef-482a-a037-9e11935a47b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>설명</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>문자</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>＼＼</th>\n",
       "      <td>역슬래시 자체를 의미</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>＼d</th>\n",
       "      <td>모든 숫자를 의미, [0-9]와 동일</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>＼D</th>\n",
       "      <td>숫자를 제외한 모든 문자를 의미, [^0-9]와 동일</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>＼s</th>\n",
       "      <td>공백을 의미, [＼t,＼n,＼f,＼v]와 동일</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>＼S</th>\n",
       "      <td>공백을 제외한 모든 문자를 의미, [^ ＼t,＼n＼f,＼v ]와 동일</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>＼w</th>\n",
       "      <td>문자와 숫자를 의미, [a-zA-Z0-9]와 동일</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>＼W</th>\n",
       "      <td>문자와 숫자를 제외한 다른 문자를 의미, [^ a-zA-Z0-9]와 동일</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          설명\n",
       "문자                                          \n",
       "＼＼                               역슬래시 자체를 의미\n",
       "＼d                      모든 숫자를 의미, [0-9]와 동일\n",
       "＼D             숫자를 제외한 모든 문자를 의미, [^0-9]와 동일\n",
       "＼s                 공백을 의미, [＼t,＼n,＼f,＼v]와 동일\n",
       "＼S    공백을 제외한 모든 문자를 의미, [^ ＼t,＼n＼f,＼v ]와 동일\n",
       "＼w               문자와 숫자를 의미, [a-zA-Z0-9]와 동일\n",
       "＼W  문자와 숫자를 제외한 다른 문자를 의미, [^ a-zA-Z0-9]와 동일"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "역슬래시문자규칙"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea05159-d9f7-4e73-adc5-e4959b704f4f",
   "metadata": {},
   "source": [
    "#### 2-2-1 match"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea7c061-16a5-407a-940c-277daa7c7449",
   "metadata": {},
   "source": [
    "###### 컴파일한 정규 표현식을 이용해 문자열이 정규 표현식과 맞는지 검사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca084295-777c-4fac-b8f4-7e68cc65a0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ec7887d-e957-4881-9100-ed9b071a6e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 3), match='abc'>\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "check = \"ab.\"\n",
    "print(re.match(check, \"abc\"))\n",
    "print(re.match(check, \"c\"))\n",
    "print(re.match(check, \"ab\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c6dc29-c1b0-4b2e-a498-485f65f3db20",
   "metadata": {},
   "source": [
    "#### 2-2-2 compile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6548b63e-45ac-4b14-bb8a-24e5b76e8639",
   "metadata": {},
   "source": [
    "###### compile을 사용하면 여러 번 사용할 경우 일반 사용보다 더 빠른 속도를 보임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddcdb43-7371-44ff-9f1d-281084ef8b11",
   "metadata": {},
   "source": [
    "###### compile을 통해 정규 표현식을 사용할 경우 re가 아닌 컴파일한 객체 이름을 통해 사용해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06c9b3c2-d8a4-45d3-bdca-da95e9a3562f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "일반 사용시 소요 시간:  0.013529777526855469\n",
      "컴파일 사용시 소요 시간:  0.0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "normal_s_time = time.time()\n",
    "r= \"ab.\"\n",
    "for i in range(10000):\n",
    "    re.match(check,\"abc\")\n",
    "print(\"일반 사용시 소요 시간: \", time.time()-normal_s_time)\n",
    "\n",
    "compile_s_time = time.time()\n",
    "r = re.compile(\"ab.\")\n",
    "for i in range(10000):\n",
    "    r.match(check)\n",
    "print(\"컴파일 사용시 소요 시간: \", time.time() - compile_s_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa002859-5735-4dc0-b287-69c48b1c129d",
   "metadata": {},
   "source": [
    "#### 2-2-3 search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5683cc1b-5d7b-4606-85cf-5ead5b4f62a0",
   "metadata": {},
   "source": [
    "###### match와 다르게, search는 문자열의 전체를 검사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dbcf0921-f3f4-4428-8e5c-ea54f91249a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 1), match='a'>\n",
      "None\n",
      "None\n",
      "<re.Match object; span=(0, 2), match='ab'>\n"
     ]
    }
   ],
   "source": [
    "check = \"ab?\"\n",
    "\n",
    "print(re.search(\"a\", check))\n",
    "print(re.match(\"kkkab\", check))\n",
    "print(re.search(\"kkkab\", check))\n",
    "print(re.match(\"ab\",check))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15efd7c6-8bac-490d-bc8c-48a7f984a94a",
   "metadata": {},
   "source": [
    "#### 2-2-4 split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b986491f-0bac-488b-9674-3ee15ca33bb7",
   "metadata": {},
   "source": [
    "###### 정규표현식에 해당하는 문자열을 기준으로 문자열을 나눔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "32483ba2-7fcd-45d8-a7b6-401b1c52881a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abc', 'abbc', 'abcbab']\n",
      "['ab', ' abb', ' ab', 'bab']\n",
      "['s', 'abc ', 'v', 's ', 'sss ', 'a']\n"
     ]
    }
   ],
   "source": [
    "r = re.compile(\" \")\n",
    "print(r.split(\"abc abbc abcbab\"))\n",
    "\n",
    "r = re.compile(\"c\")\n",
    "print(r.split(\"abc abbc abcbab\"))\n",
    "\n",
    "r = re.compile(\"[1-9]\")\n",
    "print(r.split(\"s1abc 2v3s 4sss 5a\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee591fc-c12c-4d57-b487-3dbad91090da",
   "metadata": {},
   "source": [
    "#### 2-2-5 sub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7249fba3-c817-413f-9d29-0474e65d5b5e",
   "metadata": {},
   "source": [
    "###### 정규 표현식과 일치하는 부분을 다른 문자열로 교체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "243bd46e-5995-4045-a70b-ba23fb10f0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "abc defg\n"
     ]
    }
   ],
   "source": [
    "print(re.sub(\"[a-z]\",\"abcdefg\",\"1\"))\n",
    "\n",
    "print(re.sub('[^a-z]','abc defg',\"1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7578761c-b868-4a3f-8852-6b5f4b936ecd",
   "metadata": {},
   "source": [
    "#### 2-2-6 findall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663e9ec3-23a7-4433-86ed-df584627c764",
   "metadata": {},
   "source": [
    "###### 컴파일한 정규 표현식을 이용해 정규 표현식과 맞는 모든 문자(열)을 리스트로 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c4f6a80-464b-443e-bc62-c424dc681cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2', '3', '4']\n",
      "['!', '@', '@', '#']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(\"[\\d]\", \"1ab 2cd 3ef 4g\"))\n",
    "\n",
    "print(re.findall('[\\W]', \"!abcd@@#\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540bf6d6-e0e5-4654-8c31-3a55795a314c",
   "metadata": {},
   "source": [
    "#### 2-2-7 finditer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b71b57-435b-440c-9cb6-2881596f00dd",
   "metadata": {},
   "source": [
    "###### 컴파일한 정규 표현식을 이용해 정규 표현식과 맞는 모든 문자(열)을 iterator 객체로 반환\n",
    "###### iterator 객체를 이용하면 생성된 객체를 하나씩 자동으로 가져올 수 있어 처리가 간편함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d90e065c-c377-4646-b4c5-2230778369dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<callable_iterator object at 0x0000025176391820>\n",
      "<re.Match object; span=(0, 1), match='1'>\n",
      "<re.Match object; span=(4, 5), match='2'>\n",
      "<re.Match object; span=(8, 9), match='3'>\n",
      "<re.Match object; span=(12, 13), match='4'>\n",
      "<callable_iterator object at 0x0000025176391AF0>\n",
      "<re.Match object; span=(0, 1), match='!'>\n",
      "<re.Match object; span=(5, 6), match='$'>\n",
      "<re.Match object; span=(6, 7), match='$'>\n",
      "<re.Match object; span=(7, 8), match='#'>\n"
     ]
    }
   ],
   "source": [
    "iter1 = re.finditer('[\\d]', '1ab 2cd 3ef 4g')\n",
    "print(iter1)\n",
    "for i in iter1:\n",
    "    print(i)\n",
    "    \n",
    "iter2 = re.finditer('[\\W]', '!abcd$$#')\n",
    "print(iter2)\n",
    "for i in iter2:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbaa1ad4-0255-43c1-85fe-e8592db40dfd",
   "metadata": {},
   "source": [
    "### 2-3 토큰화(Tokenization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a1136e-bec8-42bf-a1fd-f05de635e472",
   "metadata": {},
   "source": [
    "##### --특수문자에 대한 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6737bf-8f49-4da2-9da4-f3fd25ee6559",
   "metadata": {},
   "source": [
    "###### ------- 1. 단어에 일반적으로 사용되는 알파벳, 숫자와는 다르게 특수분자는 별도의 처리가 필요\n",
    "###### ------- 2. 일괄적으로 단어의 특수문자를 제거하는 방법도 있지만 특수문자가 단어에 특별한 의미를 가질 때 이를 학습에 반영시키지 못할 수도 있음\n",
    "###### ------- 3. 특수문자에 대한 일괄적인 제거보다는 데이터의 특성을 파악하고, 처리를 하는 것이 중요"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c795667-de1e-42b9-8882-6d2952ba0ef1",
   "metadata": {},
   "source": [
    "##### --특정단어에 대한 토큰 분리 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa42574-8045-4a2f-b7cf-cbeba1608a50",
   "metadata": {},
   "source": [
    "###### ------- 1. 한 단어지만 토큰으로 분리할 때 판단되는 문자들로 이루어진 we're, United Kingdom 등의 단어는 어떻게 분리해야 할지 선택이 필요 \n",
    "###### ------- 2. we're은 한 단어이나 분리해도 단어의 의미에 별 영향을 끼치진 않지만 United Kingdom은 두 단어가 모여 특정 의미를 가리켜 분리해선 안됨\n",
    "###### ------- 3. 사용자가 단어의 특성을 고려해 토큰을 분리하는 것이 학습에 유리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b8d911-2f3a-4839-9e61-6ee98d9dba17",
   "metadata": {},
   "source": [
    "#### 2-3-1 단어 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011cd00a-d228-4bc2-8826-c5451dfddc6b",
   "metadata": {},
   "source": [
    "###### 파이썬 내장 함수인 split을 활용해 단어 토큰화\n",
    "###### 공백을 기준으로 단어를 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab741f0a-38fb-471c-abd2-ee4c4c01e6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Time', 'is', 'gold']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Time is gold\"\n",
    "tokens = [x for x in sentence.split(\" \")]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f9afbb-ae88-4374-9622-f3d5251be9bb",
   "metadata": {},
   "source": [
    "###### 토큰화는 nltk 패키지의 tokenize 모듈을 사용해 손쉽게 구현 가능\n",
    "###### 단어 토큰화는 word_tokenize()함수를 사용해 구현가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "75552a29-7c0a-4520-a1ac-45b3621a6ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "740ceeb2-17f9-4b27-91b2-44bb79c2ad21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Time', 'is', 'gold']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5a777a-d0ac-4287-bd56-331c1e07b1f9",
   "metadata": {},
   "source": [
    "#### 2-3-2 문장 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01a8e81-ad06-4b0e-9f4b-cf680ef98042",
   "metadata": {},
   "source": [
    "###### 문장 토큰화는 줄바꿈 문자(\"\\n\")를 기준으로 문장을 분리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "48bb2b76-fa94-44bb-af81-ec5d11cfe822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The world is a beatutiful book.\n",
      "But of little use to him who cannot read it.\n",
      "['The world is a beatutiful book.', 'But of little use to him who cannot read it.']\n"
     ]
    }
   ],
   "source": [
    "sentences = \"The world is a beatutiful book.\\nBut of little use to him who cannot read it.\"\n",
    "print(sentences)\n",
    "\n",
    "tokens = [x for x in sentences.split(\"\\n\")]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38efbb1e-c4b8-4159-bcec-1d1002ad05cc",
   "metadata": {},
   "source": [
    "###### 문장 토큰화는 sent_tokenize()함수를 사용해 구현 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "595b8618-34e7-40d4-9560-0f8309c8ad61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The world is a beatutiful book.', 'But of little use to him who cannot read it.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "tokens = sent_tokenize(sentences)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1afad50-6469-4985-a9ac-b576a76df665",
   "metadata": {},
   "source": [
    "###### 문장 토큰화에서는 온점(.)의 처리를 위해 이진 분류기를 사용할 수도 있음\n",
    "###### 온점은 문장과 문장을 구분해줄 수도, 문장에 포함된 단어를 구성할 수도 있기 때문에 이를 이진 분류기로 분류해 더욱 좋은 토큰화를 구현할 수도 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a5de2e-b32e-46f0-ab72-ef83c313fa09",
   "metadata": {},
   "source": [
    "#### 2-3-3 정규 표현식을 이용한 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f368aa5-318f-4864-8372-2e5c6e2756bf",
   "metadata": {},
   "source": [
    "###### 토큰화 기능을 직접 구현할 수도 있지만 정규 표현식을 이용해 간단하게 구현할 수도 있음\n",
    "###### nltk 패키지는 정규 표현식을 사용하는 토큰화 도구인 RegexpTokenizer를 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3d2a33b7-4a53-464f-a780-1f0efdea15ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Where', 'there', 's', 'a', 'will', 'there', 's', 'a', 'way']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "sentence = \"Where there\\'s a will, there\\'s a way\"\n",
    "\n",
    "tokenizer = RegexpTokenizer(\"[\\w]+\")\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4a1551f4-8818-4ed7-aba8-3464869b2d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Where', \"there's\", 'a', 'will,', \"there's\", 'a', 'way']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(\"[\\s]+\", gaps=True)\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b513e8-c32f-424c-8aea-e17559d50cdb",
   "metadata": {},
   "source": [
    "#### 2-3-4 케라스를 이용한 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fd05ff68-8485-4bcd-8681-0ddc132a3699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['where', \"there's\", 'a', 'will', \"there's\", 'a', 'way']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "sentence = \"Where there\\'s a will, there\\'s a way\"\n",
    "\n",
    "text_to_word_sequence(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c7f530-280f-425d-a6be-1bdb7f455423",
   "metadata": {},
   "source": [
    "#### 2-3-5 TextBlob을 이용한 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "364049b4-cc16-491a-9b6d-b416132d1411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Where', 'there', \"'s\", 'a', 'will', 'there', \"'s\", 'a', 'way']\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "sentence = \"Where there\\'s a will, there\\'s a way\"\n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "\n",
    "print(blob.words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83b20a1-a68b-456f-a30f-34dcc9122798",
   "metadata": {},
   "source": [
    "#### 2-3-6 기타 토크나이저"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f47be78-9edb-4677-9b08-880ad5b1e831",
   "metadata": {},
   "source": [
    "###### ---- 1. WhiteSpaceTokenizer : 공백을 기준으로 토큰화\n",
    "###### ---- 2. WordPunktTokenizer : 텍스트를 알파벳 문자, 숫자, 알파벳 이외의 문자 리스트로 토큰화\n",
    "###### ---- 3. MWETokenizer : MWE는 Multi-Word Expression의 약자로 'repulblic of korea'와 같이 여러 단어로 이루어진 특정 그룹을 한개체로 취급\n",
    "###### ---- 4. TweetTokenizer : 트위터에서 사용되는 문자의 토큰화를 위해서 만들어졌으며, 문장 속 감성의 표현과 감정을 다룸"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85909be6-d661-41b5-b0f4-1c60233551f1",
   "metadata": {},
   "source": [
    "#### 2-4 n-gram 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e72d73-c6bc-4849-9baf-ceb2da46593f",
   "metadata": {},
   "source": [
    "###### n-gram은 n개의 어절이나 음절을 연쇄적으로 분류해 그 빈도를 분석\n",
    "###### n=1일 때는 unigram, n=2일 때는 bigram, n=3일 때는 trigram으로 불림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b75d7db8-0494-49bf-a607-53fb212ead8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('There', 'is'), ('is', 'no'), ('no', 'royal'), ('royal', 'road'), ('road', 'to'), ('to', 'learning')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "sentence = \"There is no royal road to learning\"\n",
    "bigram = list(ngrams(sentence.split(),2))\n",
    "print(bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "36d7e540-01ba-4621-ad1f-5a852dd243da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('There', 'is', 'no'), ('is', 'no', 'royal'), ('no', 'royal', 'road'), ('royal', 'road', 'to'), ('road', 'to', 'learning')]\n"
     ]
    }
   ],
   "source": [
    "trigram = list(ngrams(sentence.split(),3))\n",
    "print(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9bc25e9e-d486-43bb-a8ce-37cc6920be1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['There', 'is']),\n",
       " WordList(['is', 'no']),\n",
       " WordList(['no', 'royal']),\n",
       " WordList(['royal', 'road']),\n",
       " WordList(['road', 'to']),\n",
       " WordList(['to', 'learning'])]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "blob.ngrams(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cf806149-c636-4790-8155-9287572bebb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['There', 'is', 'no']),\n",
       " WordList(['is', 'no', 'royal']),\n",
       " WordList(['no', 'royal', 'road']),\n",
       " WordList(['royal', 'road', 'to']),\n",
       " WordList(['road', 'to', 'learning'])]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.ngrams(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd039728-c5a9-4e32-8478-3af97b262e6c",
   "metadata": {},
   "source": [
    "#### 2-5 PoS(Parts of Speech) 태깅"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d20876f-6cff-45c5-8337-b04eb758067f",
   "metadata": {},
   "source": [
    "###### PoS는 품사를 의미하며, PoS 태깅은 문장 내에서 단어에 해당하는 각 품사를 태깅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "378b0b9f-6bfa-4468-b2ba-1053e65ee328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cd576043-9d5c-42bf-9952-adde50d2eeec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Think',\n",
       " 'like',\n",
       " 'man',\n",
       " 'of',\n",
       " 'action',\n",
       " 'and',\n",
       " 'act',\n",
       " 'like',\n",
       " 'man',\n",
       " 'of',\n",
       " 'thought',\n",
       " '.']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = word_tokenize(\"Think like man of action and act like man of thought.\")\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cda4d1e5-b8cf-442d-a436-ff89b3767a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\DS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"averaged_perceptron_tagger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d7b33d76-a380-4adf-892e-0ab7908550b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Think', 'VBP'),\n",
       " ('like', 'IN'),\n",
       " ('man', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('action', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('act', 'NN'),\n",
       " ('like', 'IN'),\n",
       " ('man', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('thought', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a6ac077d-721b-4ace-bc9f-92a5a6f31382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 'DT'),\n",
       " ('rolling', 'VBG'),\n",
       " ('stone', 'NN'),\n",
       " ('gatehrs', 'NN'),\n",
       " ('no', 'DT'),\n",
       " ('moss', 'NN')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(word_tokenize(\"A rolling stone gatehrs no moss\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2b65a0-57ec-4ae9-9b75-44dacb43bd1e",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### - PoS 태그리스트 -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c44dc4f3-15be-411c-b27b-ca604f0deace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tag</th>\n",
       "      <th>Description</th>\n",
       "      <th>설명</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Number</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CC</td>\n",
       "      <td>Coordinatting conjunction</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CD</td>\n",
       "      <td>Cardinal number</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DT</td>\n",
       "      <td>Determiner</td>\n",
       "      <td>한정사</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EX</td>\n",
       "      <td>Existential there</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FW</td>\n",
       "      <td>Foreign word</td>\n",
       "      <td>외래어</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>IN</td>\n",
       "      <td>Preposition or subordinating conjunction</td>\n",
       "      <td>전치사 또는 종속 접속사</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>JJ</td>\n",
       "      <td>Adjective</td>\n",
       "      <td>형용사</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>JJR</td>\n",
       "      <td>Adjective,comparative</td>\n",
       "      <td>형용사,비교급</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>JJS</td>\n",
       "      <td>Adjective,superlative</td>\n",
       "      <td>형용사,최상급</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LS</td>\n",
       "      <td>List item marker</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>MD</td>\n",
       "      <td>Modal</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NN</td>\n",
       "      <td>Noun,singular or mass</td>\n",
       "      <td>명사,단수형</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NNS</td>\n",
       "      <td>Noun,plural</td>\n",
       "      <td>명사,복수형</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NNP</td>\n",
       "      <td>Proper noun,singular</td>\n",
       "      <td>고유명사,단수형</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NNPS</td>\n",
       "      <td>Proper noun,plural</td>\n",
       "      <td>고유명사,복수형</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>PDT</td>\n",
       "      <td>Predeterminer</td>\n",
       "      <td>전치한정사</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>POS</td>\n",
       "      <td>Possessive ending</td>\n",
       "      <td>소유형용사</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>PRP</td>\n",
       "      <td>Personal pronoun</td>\n",
       "      <td>인칭 대명사</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>PRP$</td>\n",
       "      <td>Possessive pronoun</td>\n",
       "      <td>소유 대명사</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>RB</td>\n",
       "      <td>Adverb</td>\n",
       "      <td>부사</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>RBR</td>\n",
       "      <td>Adverb,comparative</td>\n",
       "      <td>부사,비교급</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>RBS</td>\n",
       "      <td>Adverb,superlative</td>\n",
       "      <td>부사,최상급</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>RP</td>\n",
       "      <td>Particle</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>SYM</td>\n",
       "      <td>Symbol</td>\n",
       "      <td>기호</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>TO</td>\n",
       "      <td>to</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>UH</td>\n",
       "      <td>Interjection</td>\n",
       "      <td>감탄사</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>VB</td>\n",
       "      <td>Verb,base form</td>\n",
       "      <td>동사,원형</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>VBD</td>\n",
       "      <td>Verb,past tense</td>\n",
       "      <td>동사,과거형</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>VBG</td>\n",
       "      <td>Verb, gerund or present participle</td>\n",
       "      <td>동사,현재분사</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>VBN</td>\n",
       "      <td>Verb, past participle</td>\n",
       "      <td>동사,과거분사</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>VBP</td>\n",
       "      <td>Verb, non-3rd person singular present</td>\n",
       "      <td>동사,비3인칭 단수</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>VBZ</td>\n",
       "      <td>Verb, 3rd person singular present</td>\n",
       "      <td>동사3인칭 단수</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>WDT</td>\n",
       "      <td>Wh-determiner</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>WP</td>\n",
       "      <td>Wh-pronoun</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>WP$</td>\n",
       "      <td>Possessive wh-pronoun</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>WRB</td>\n",
       "      <td>Wh-adverb</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Tag                               Description             설명\n",
       "Number                                                               \n",
       "1         CC                 Coordinatting conjunction            NaN\n",
       "2         CD                           Cardinal number            NaN\n",
       "3         DT                                Determiner            한정사\n",
       "4         EX                         Existential there            NaN\n",
       "5         FW                              Foreign word            외래어\n",
       "6         IN  Preposition or subordinating conjunction  전치사 또는 종속 접속사\n",
       "7         JJ                                 Adjective            형용사\n",
       "8        JJR                     Adjective,comparative        형용사,비교급\n",
       "9        JJS                     Adjective,superlative        형용사,최상급\n",
       "10        LS                          List item marker            NaN\n",
       "11        MD                                     Modal            NaN\n",
       "12        NN                     Noun,singular or mass         명사,단수형\n",
       "13       NNS                               Noun,plural         명사,복수형\n",
       "14       NNP                      Proper noun,singular       고유명사,단수형\n",
       "15      NNPS                        Proper noun,plural       고유명사,복수형\n",
       "16       PDT                             Predeterminer          전치한정사\n",
       "17       POS                         Possessive ending          소유형용사\n",
       "18       PRP                          Personal pronoun         인칭 대명사\n",
       "19      PRP$                        Possessive pronoun         소유 대명사\n",
       "20        RB                                    Adverb             부사\n",
       "21       RBR                        Adverb,comparative         부사,비교급\n",
       "22       RBS                        Adverb,superlative         부사,최상급\n",
       "23        RP                                  Particle            NaN\n",
       "24       SYM                                    Symbol             기호\n",
       "25        TO                                        to            NaN\n",
       "26        UH                              Interjection            감탄사\n",
       "27        VB                            Verb,base form          동사,원형\n",
       "28       VBD                           Verb,past tense         동사,과거형\n",
       "29       VBG        Verb, gerund or present participle        동사,현재분사\n",
       "30       VBN                     Verb, past participle        동사,과거분사\n",
       "31       VBP     Verb, non-3rd person singular present     동사,비3인칭 단수\n",
       "32       VBZ         Verb, 3rd person singular present       동사3인칭 단수\n",
       "33       WDT                             Wh-determiner            NaN\n",
       "34        WP                                Wh-pronoun            NaN\n",
       "35       WP$                     Possessive wh-pronoun            NaN\n",
       "36       WRB                                 Wh-adverb            NaN"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PoS태그리스트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdab140-dd05-4903-bbb8-61896c734aa7",
   "metadata": {},
   "source": [
    "### 2-6. 불용어 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80174990-ebb5-4a36-97e3-e30797b14e85",
   "metadata": {},
   "source": [
    "###### 1 : 영어의 전치사(on, in), 한국어의 조사(을,를)등은 분석에 필요하지 않은 경우가 많음\n",
    "###### 2 : 길이가 짧은 단어, 등장 빈도 수가 적은 단어들도 분석에 큰 영향을 주지 않음\n",
    "###### 3 : 일반적으로 사용되는 도구들은 해당 단어들을 제거해주지만 완벽하게 제거되지는 않음\n",
    "###### 4 : 사용자가 불용어 사전을 만들어 해당 단어들을 제거하는 것이 좋음\n",
    "###### 5 : 도구들이 걸러주지 않는 전치사, 조사 등을 불용어 사전을 만들어 불필요한 단어들을 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cc843fa4-fb47-4b0b-91c5-16c913e191c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['on', 'in', 'the']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = \"on in the\"\n",
    "stop_words = stop_words.split(\" \")\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7239cfce-e2f3-413c-b5f3-0a6900ee682f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['singer', 'stage']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"singer on the stage\"\n",
    "sentence = sentence.split(\" \")\n",
    "nouns = []\n",
    "for noun in sentence:\n",
    "    if noun not in stop_words:\n",
    "        nouns.append(noun)\n",
    "        \n",
    "print(nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13a7172-8539-4399-813b-7ddb61108f33",
   "metadata": {},
   "source": [
    "###### nltk 패키지에 불용어 리스트 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3827cb0c-d383-432a-8b78-efdf910b85de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "73a02ed6-8be2-4f5a-b83a-63d754c57d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4b423f2c-93cf-42ec-9e65-f7df2121106a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stop_words = stopwords.words(\"english\")\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7f14fc81-e7fa-451c-aea7-00cf589d8e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['If', 'you', 'do', 'not', 'walk', 'today', ',', 'you', 'will', 'have', 'to', 'run', 'tomorrow', '.']\n"
     ]
    }
   ],
   "source": [
    "s = \"If you do not walk today, you will have to run tomorrow.\"\n",
    "words = word_tokenize(s)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bd02e073-dae8-47c5-8767-516da6e73f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['If', 'walk', 'today', ',', 'run', 'tomorrow', '.']\n"
     ]
    }
   ],
   "source": [
    "no_stopwords = []\n",
    "for w in words:\n",
    "    if w not in stop_words:\n",
    "        no_stopwords.append(w)\n",
    "        \n",
    "print(no_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a045f2c-cf50-4a15-a178-63354de7f4bd",
   "metadata": {},
   "source": [
    "### 2-7. 철자 교정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e2f86b-65ea-4a13-9a8d-3fa1b65f0e92",
   "metadata": {},
   "source": [
    "###### 1 : 텍스트에 오탈자가 존재하는 경우가 있음\n",
    "###### 2 : 예를 들어, 단어 \"apple\"을 \"aplpe\"과 같이 철자 순서가 바뀌거나 supple 같이 철자가 틀릴 수 있음\n",
    "###### 3 : 사람이 적절한 추정을 통해 이해하는데는 문제가 없지만, 컴퓨터는 이러한 단어를 그대로 받아들여 처리가 필요\n",
    "###### 4 : 철자 교정 알고리즘은 이미 개발되어 워드 프로세서나 다양한 서비스에서 많이 적용됨\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5cdc61b8-4fe3-49be-818f-55378a36fe17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: autocorrect in c:\\users\\ds\\anaconda3\\lib\\site-packages (2.6.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install autocorrect\n",
    "from autocorrect import Speller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c6d49a7b-adb8-4c41-a0aa-0af8d2d190a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people\n",
      "people\n",
      "people\n"
     ]
    }
   ],
   "source": [
    "spell = Speller(\"en\")\n",
    "\n",
    "print(spell(\"peoplle\"))\n",
    "print(spell(\"peope\"))\n",
    "print(spell(\"peopae\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "288b2bcf-dcfe-45be-88e3-ba7699eda9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Earlly', 'biird', 'catchess', 'the', 'womm', '.']\n",
      "Early bird catches the worm .\n"
     ]
    }
   ],
   "source": [
    "s = word_tokenize(\"Earlly biird catchess the womm.\")\n",
    "print(s)\n",
    "ss =  \" \".join([spell(s) for s in s])\n",
    "print(ss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b24a51-1ca1-4bb4-b173-37d47eba001d",
   "metadata": {},
   "source": [
    "### 2-8. 언어의 단수화와 복수화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c1462696-9739-4b83-b9fe-ee5e830907e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apples', 'bananas', 'organges']\n",
      "['apple', 'banana', 'organge']\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "words = \"apples bananas organges\"\n",
    "tb = TextBlob(words)\n",
    "print(tb.words)\n",
    "print(tb.words.singularize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fee72fb5-f56f-4045-b326-0b738809a5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['car', 'tarain', 'airplane']\n",
      "['cars', 'tarains', 'airplanes']\n"
     ]
    }
   ],
   "source": [
    "words = \"car tarain airplane\"\n",
    "tb = TextBlob(words)\n",
    "\n",
    "print(tb.words)\n",
    "print(tb.words.pluralize())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b85eaf5-2f4a-4317-beb6-05c35ce23e6a",
   "metadata": {},
   "source": [
    "### 2-9. 어간(Stemming) 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c830e114-cda9-4115-9b91-ef589374d32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "stemmer = nltk.stem.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0512e00a-f454-4140-a6f0-6e7984db026c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'applic'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"application\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0bbba022-0e4d-4dd7-8dca-776d26da08d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'begin'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"beginning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ac1abf2b-821e-43cb-930e-af972b454e81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'catch'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"catches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "802b1988-cf37-4d98-aba1-5347d8740436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'educ'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"education\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebc6e9c-005d-479d-9906-5226671abd7e",
   "metadata": {},
   "source": [
    "### 2-10. 표제어(Lemmatization) 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "beabce69-de1a-4b40-841d-1693c96a7339",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"wordnet\")\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "16c87965-c9c1-4402-a71a-85ab28dccc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\DS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "465d6546-d347-43a8-8528-3dbf92942f4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'application'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"application\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "97262787-4121-4bd5-8e7c-110d167eb07f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'beginning'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"beginning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b4db386b-cb0b-4cef-98e9-5051481a9036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'catch'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"catches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d755eb55-6f9a-4cfc-9f5c-5ba27b932ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'education'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"education\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe850f22-da7d-4ba6-8c83-dabf30f1b288",
   "metadata": {},
   "source": [
    "### 2-11. 개체명 인식(Named Entity Recognition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f3e31bc4-fa6f-4608-9c31-e9ec139960ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\DS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\DS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "nltk.download(\"maxent_ne_chunker\")\n",
    "nltk.download(\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cf9d5880-48d9-444c-9e18-bc7f81d3a93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rome was not bulit in a day\n"
     ]
    }
   ],
   "source": [
    "s = \"Rome was not bulit in a day\"\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6202d8bd-f65f-4339-a8a7-2f4698e14b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Rome', 'NNP'), ('was', 'VBD'), ('not', 'RB'), ('bulit', 'VBN'), ('in', 'IN'), ('a', 'DT'), ('day', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "tags = nltk.pos_tag(word_tokenize(s))\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6dea3631-fabf-4196-9836-aaea5d8ac1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NE Rome/NNP) was/VBD not/RB bulit/VBN in/IN a/DT day/NN)\n"
     ]
    }
   ],
   "source": [
    "entities = nltk.ne_chunk(tags, binary= True)\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0176d866-3704-49e2-a68a-80be8ebb3be2",
   "metadata": {},
   "source": [
    "### 2-12. 단어 중의성(Lexical Ambiguity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "28d05e6f-06d6-4487-90d4-f06870b8067e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'saw', 'bats', '.']\n",
      "Synset('saw.v.01')\n",
      "Synset('squash_racket.n.01')\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.wsd import lesk\n",
    "\n",
    "s = \"I saw bats.\"\n",
    "\n",
    "print(word_tokenize(s))\n",
    "print(lesk(word_tokenize(s),\"saw\"))\n",
    "print(lesk(word_tokenize(s),\"bats\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d61a641-64cf-405b-806e-3cc4de7201dd",
   "metadata": {},
   "source": [
    "### 3. 한국어 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23eafc8-ae6e-48a8-be5a-1b3645b055ce",
   "metadata": {},
   "source": [
    "#### 3-1 정규 표현식"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f51aef6-d735-49f0-9b61-374a9edec6d9",
   "metadata": {},
   "source": [
    "###### 1 : 한국어 정규 표현식도 대부분의 문법은 영어 정규 표현식과 같음\n",
    "###### 2 : 한국어는 자음과 모음이 분리되어 있기 때문에, 문법을 지정할 때는 자음과 모음을 동시에 고려해야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1205016c-1d83-446a-af8c-38160120874d",
   "metadata": {},
   "source": [
    "#### 3-1-1 match"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163f5cd2-827a-4534-b887-d5e691ff67fb",
   "metadata": {},
   "source": [
    "###### 1 : 컴파일한 정규 표현식을 이용해 문자열이 정규 표현식과 맞는지 검사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bbbd0369-b303-4dc5-89a1-d622dfe520df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 1), match='ㅎ'>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "check  = \"[ㄱ-ㅎ]+\"\n",
    "print(re.match(check, \"ㅎ 안녕하세요.\"))\n",
    "print(re.match(check, \"안녕하세요. ㅎ\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1522601-c89e-4358-8ce5-0c73fe827a53",
   "metadata": {},
   "source": [
    "#### 3-1-2 search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdddecb-2436-4c8c-83b3-08020f3af4fc",
   "metadata": {},
   "source": [
    "###### 1 : match와 다르게, search는 문자열의 전체를 검사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "745f8786-8f29-4394-b968-98d3a6b502ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 2), match='ㄱㅏ'>\n",
      "None\n",
      "<re.Match object; span=(2, 4), match='ㄱㅏ'>\n"
     ]
    }
   ],
   "source": [
    "check  = \"[ㄱ-ㅎ|ㅏ-ㅣ]+\"\n",
    "\n",
    "print(re.search(check, \"ㄱㅏ 안녕하세요\"))\n",
    "print(re.match(check, \"안 ㄱㅏ\"))\n",
    "print(re.search(check, \"안 ㄱㅏ\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311e8c12-2330-4e23-8e8e-98f656cd37e5",
   "metadata": {},
   "source": [
    "#### 3-1-3 sub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7049a6-e1fc-42e8-a8e1-e1c1452aa538",
   "metadata": {},
   "source": [
    "###### 정규 표현식과 일치하는 부분을 다른 문자열로 교체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8e3e4f60-6926-4559-bee7-9d09402f60d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "가나다라마바사\n"
     ]
    }
   ],
   "source": [
    "print(re.sub(\"[가-힣]\",\"가나다라마바사\",\"1\"))\n",
    "print(re.sub(\"[^가-힣]\", \"가나다라마바사\",\"1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c906c6-cadf-45f4-8afc-9349903ea729",
   "metadata": {},
   "source": [
    "#### 3-2 토큰화(Tokenization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac42116-c0cf-45ec-a6d1-61007c8c2bcd",
   "metadata": {},
   "source": [
    "###### 1 : 한국어를 학습 데이터를 사용할 때는 언어의 특성으로 인해 추가로 고려해야 할 사항이 존재\n",
    "###### 2 : 한국어는 띄어쓰기를 준수하지 않아도 의미가 전달되는 경우가 많아 띄어쓰기가 지켜지지 않을 가능성이 존재\n",
    "###### 3 : 띄어쓰기가 지켜지지 않으면 정상적인 토큰 분리가 이루어지지 않음\n",
    "###### 4 : 한국어는 형태소라는 개념이 존재해 추가로 고려해주어야만 함\n",
    "###### 5 : '그는','그가'등의 단어들은 같은 의미를 가리키지만 텍스트 처리에서는 다르게 받아들일 수 있어 처리를 해줘야만 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779da75b-9e48-4b5f-ba29-057df1a7df05",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 한국어 자연어 처리 konlpy와 형태소 분석기 MeCab 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6ff75419-d450-475a-a79b-090cbf7533ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting eunjeon\n",
      "  Using cached eunjeon-0.4.0.tar.gz (34.7 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: eunjeon\n",
      "  Building wheel for eunjeon (setup.py): started\n",
      "  Building wheel for eunjeon (setup.py): finished with status 'done'\n",
      "  Created wheel for eunjeon: filename=eunjeon-0.4.0-cp39-cp39-win_amd64.whl size=35014101 sha256=38d347f3edac8a103e35dc05a4373701f3dfead99a7320c7495d22edd90d78c7\n",
      "  Stored in directory: c:\\users\\ds\\appdata\\local\\pip\\cache\\wheels\\51\\05\\fb\\48ff3bf6804f7cea4a3f7be6300a5b19618007c35d2064a753\n",
      "Successfully built eunjeon\n",
      "Installing collected packages: eunjeon\n",
      "Successfully installed eunjeon-0.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install eunjeon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d60431f8-415c-4c67-a62d-ae86bc64ce70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eunjeon import Mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "aa7a287a-718d-44b5-bdae-2381858ad9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = Mecab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "90d7dd21-7c6f-45e3-9d04-7c462f8b17d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('언제나', 'MAG'),\n",
       " ('현재', 'NNG'),\n",
       " ('에', 'JKB'),\n",
       " ('집중', 'NNG'),\n",
       " ('할', 'XSV+ETM'),\n",
       " ('수', 'NNB'),\n",
       " ('있', 'VV'),\n",
       " ('다면', 'EC'),\n",
       " ('행복', 'NNG'),\n",
       " ('할', 'XSV+ETM'),\n",
       " ('것', 'NNB'),\n",
       " ('이', 'VCP'),\n",
       " ('다', 'EC')]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"언제나 현재에 집중할 수 있다면 행복할 것이다\"\n",
    "tagger.pos(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ad3378-9793-4af7-bcbd-c663e9d50573",
   "metadata": {},
   "source": [
    "#### 토큰화만 실행할 때는 tagger.morphs()라는 함수를 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d994faa1-5e70-4158-b015-4d9b6c4668ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['언제나', '현재', '에', '집중', '할', '수', '있', '다면', '행복', '할', '것', '이', '다']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.morphs(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051ebc2b-9167-41ba-9eee-8617d7f77111",
   "metadata": {},
   "source": [
    "###### 형태소만 사용하고 싶을 때는 tagger.nouns()라는 함수를 이용해 조사, 접속사 등을 제가 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "53a7c583-0fa8-47be-9ba0-e6ff09804a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['현재', '집중', '수', '행복', '것']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.nouns(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "69200a84-256b-493d-bd6d-195951e50673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kss\n",
      "  Downloading kss-3.6.4.tar.gz (42.4 MB)\n",
      "     --------------------------------------- 42.4/42.4 MB 11.3 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting emoji==1.2.0\n",
      "  Downloading emoji-1.2.0-py3-none-any.whl (131 kB)\n",
      "     -------------------------------------- 131.3/131.3 kB 7.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: regex in c:\\users\\ds\\anaconda3\\lib\\site-packages (from kss) (2022.7.9)\n",
      "Collecting more_itertools\n",
      "  Downloading more_itertools-8.14.0-py3-none-any.whl (52 kB)\n",
      "     ---------------------------------------- 52.2/52.2 kB ? eta 0:00:00\n",
      "Building wheels for collected packages: kss\n",
      "  Building wheel for kss (setup.py): started\n",
      "  Building wheel for kss (setup.py): finished with status 'done'\n",
      "  Created wheel for kss: filename=kss-3.6.4-py3-none-any.whl size=42448613 sha256=0207bd67bc5b171ec731b19c7d308d559c4ae69723b26383dc7fdd42004a4337\n",
      "  Stored in directory: c:\\users\\ds\\appdata\\local\\pip\\cache\\wheels\\3c\\fd\\6f\\83961142db4df28909ad634605f93f2855d2b23e86ff3c5fde\n",
      "Successfully built kss\n",
      "Installing collected packages: emoji, more_itertools, kss\n",
      "Successfully installed emoji-1.2.0 kss-3.6.4 more_itertools-8.14.0\n"
     ]
    }
   ],
   "source": [
    "!pip install kss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6fdc73-6070-4675-812e-76730e02caf4",
   "metadata": {},
   "source": [
    "###### 라이브러리를 이용해도 한국어에는 전치 표현이 존재해 재대로 토큰화가 안됨\n",
    "###### 좀 더 나은 학습을 위해 사용자는 해당 부분을 따로 처리해주어야만 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "cce77f9d-90e4-4eff-ba7a-ccfdde32366e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Korean Sentence Splitter]: Initializing Pynori...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['진짜? 내일 뭐하지.', '이렇게 애매모호한 문장도? 밥은 먹었어?', '나는...']\n"
     ]
    }
   ],
   "source": [
    "import kss\n",
    "\n",
    "text = \"진짜? 내일 뭐하지. 이렇게 애매모호한 문장도? 밥은 먹었어? 나는...\"\n",
    "print(kss.split_sentences(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd13d9c-90fd-4b2a-ac92-9e0370d5d290",
   "metadata": {},
   "source": [
    "#### 3-2-1 정규 표현식을 이용한 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f3aa63-20d8-4174-869b-649155bc8d08",
   "metadata": {},
   "source": [
    "###### 한국어도 정규 표현식을 이용해 토큰화 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "178417af-0658-4f16-a6d0-a87e4022a16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['안녕하세요', '저는', '자연어', '처리', '를', '배우고', '있습니다']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "sentence = \"안녕하세요 ㅋㅋ 저는 자연어 처리(Natural Language Processing)를ㄹ!! 배우고 있습니다.\"\n",
    "tokenizer = RegexpTokenizer(\"[가-힣]+\")\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4b1cc0d9-c14b-4ee2-bc88-8b8d4e087569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['안녕하세요 ', ' 저는 자연어 처리(Natural Language Processing)를', '!! 배우고 있습니다.']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(\"[ㄱ-ㅎ]+\", gaps = True)\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538ec701-b12a-4ebc-871e-36b139f935ab",
   "metadata": {},
   "source": [
    "#### 3-2-2 케라스를 이용한 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e952c027-d626-4736-9324-341409d09489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['성공의', '비결은', '단', '한', '가지', '잘할', '수', '있는', '일에', '광적으로', '집중하는', '것이다']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "sentence = \"성공의 비결은 단 한 가지, 잘할 수 있는 일에 광적으로 집중하는 것이다.\"\n",
    "text_to_word_sequence(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c697f23d-f120-46b1-8478-e27ba2877197",
   "metadata": {},
   "source": [
    "#### 3-2-3 TextBlob을 이용한 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2f0e0c33-8505-46e2-ac1a-6737fb631bf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['성공의', '비결은', '단', '한', '가지', '잘할', '수', '있는', '일에', '광적으로', '집중하는', '것이다'])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "blob = TextBlob(sentence)\n",
    "blob.words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf9dfa0-0782-4d97-8f33-1f3eca619ad1",
   "metadata": {},
   "source": [
    "### 4. Bag of Words(BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5417f4d9-07ce-467e-acfd-7cdf74dcc7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 2 2 2 1 1]]\n",
      "{'think': 6, 'like': 3, 'man': 4, 'of': 5, 'action': 1, 'and': 2, 'act': 0, 'thought': 7}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\"Think like a man of action and act like man of thought.\"]\n",
    "\n",
    "vector = CountVectorizer()\n",
    "bow = vector.fit_transform(corpus)\n",
    "\n",
    "print(bow.toarray())\n",
    "print(vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6ac08b6f-55f8-4ef7-ab94-3804f5704132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 2 2 1 1]]\n",
      "{'think': 4, 'like': 2, 'man': 3, 'action': 1, 'act': 0, 'thought': 5}\n"
     ]
    }
   ],
   "source": [
    "vector = CountVectorizer(stop_words=\"english\")\n",
    "bow =  vector.fit_transform(corpus)\n",
    "\n",
    "print(bow.toarray())\n",
    "print(vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d33fb3e1-eb6d-4d27-8168-9fae51c8c327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 1 1 1 1 1 1 1 1]]\n",
      "{'평생': 8, '것처럼': 0, '꿈을': 3, '꾸어라': 2, '그리고': 1, '내일': 4, '죽을': 7, '오늘을': 6, '살아라': 5}\n"
     ]
    }
   ],
   "source": [
    "corpus = [\"평생 살 것처럼 꿈을 꾸어라. 그리고 내일 죽을 것처럼 오늘을 살아라.\"]\n",
    "\n",
    "vector = CountVectorizer()\n",
    "bow = vector.fit_transform(corpus)\n",
    "\n",
    "print(bow.toarray())\n",
    "print(vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "910a7625-781b-4a42-a647-dd31c81d6e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 2, 2, 1, 3, 1, 1, 1, 1, 1, 1, 1]\n",
      "{'평생': 0, '살': 1, '것': 2, '처럼': 3, '꿈': 4, '을': 5, '꾸': 6, '어라': 7, '그리고': 8, '내일': 9, '죽': 10, '오늘': 11, '아라': 12}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from eunjeon import Mecab\n",
    "tagger = Mecab()\n",
    "\n",
    "corpus = \"평생 살 것처럼 꿈을 꾸어라. 그리고 내일 죽을 것처럼 오늘을 살아라.\"\n",
    "tokens = tagger.morphs(re.sub(\"(\\.)\", \"\", corpus))\n",
    "\n",
    "vocab={}\n",
    "bow = []\n",
    "\n",
    "for tok in tokens:\n",
    "    if tok not in vocab.keys():\n",
    "        vocab[tok] = len(vocab)\n",
    "        bow.insert(len(vocab)-1,1)\n",
    "    else:\n",
    "        index = vocab.get(tok)\n",
    "        bow[index] = bow[index]+1\n",
    "\n",
    "print(bow)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8463af-62c5-4ea6-bcd9-3d91ef75fb57",
   "metadata": {},
   "source": [
    "### 5. 문서 단어 행렬(DTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf94b4a-86a9-4577-aebb-519341e92340",
   "metadata": {},
   "source": [
    "###### 1 : 문서 단어 행렬(Documnet-Term Matrix)은 문서에 등장하는 여러 단어들의 빈도를 행렬로 표현\n",
    "###### 2 : 각 문서에 대한 BoW를 하나의 행렬로 표현한 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "70984589-6821-4968-89e2-8dce12fda5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 0 2 2 0 1 1 0 0]\n",
      " [0 0 0 0 0 2 1 0 0 2 1]\n",
      " [0 0 1 1 0 0 0 0 0 0 0]]\n",
      "{'think': 7, 'like': 4, 'man': 5, 'action': 1, 'act': 0, 'thought': 8, 'try': 9, 'success': 6, 'value': 10, 'liberty': 3, 'death': 2}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus  = [\"Think like a man of action and act like man of thought.\",\n",
    "           \"Try not to become a man of success but rather try to become a man of value.\",\n",
    "           \"Give me liberty, of give me death.\"]\n",
    "\n",
    "vector = CountVectorizer(stop_words = \"english\")\n",
    "bow = vector.fit_transform(corpus)\n",
    "\n",
    "print(bow.toarray())\n",
    "print(vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d5d0e887-c852-4a02-aa48-4ea18f5e2da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   act  action  death  liberty  like  man  success  think  thought  try  value\n",
      "0    1       1      0        0     2    2        0      1        1    0      0\n",
      "1    0       0      0        0     0    2        1      0        0    2      1\n",
      "2    0       0      1        1     0    0        0      0        0    0      0\n"
     ]
    }
   ],
   "source": [
    "columns = []\n",
    "for k, v in sorted(vector.vocabulary_.items(), key=lambda item: item[1]):\n",
    "    columns.append(k)\n",
    "    \n",
    "df = pd.DataFrame(bow.toarray(), columns = columns)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb41a9ce-af4a-407b-86e0-598a99c1a190",
   "metadata": {},
   "source": [
    "### 6. 어휘 빈도-문서 역빈도(TF-IDF)분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546b490b-f4fa-488a-a9ab-b7c2f20032ae",
   "metadata": {},
   "source": [
    "###### 1 : 어휘 빈도- 문서 역빈도(TF-IDF; Term Frequency-Inverse Docunment Frequency)는 단순히 빈도수가 높은 단어가 핵심어가 아닌, 특정 문서에서만 집중적으로 등장할 때 해당 단어가 문서의 주제를 잘 담고 있는 핵심이라고 가정\n",
    "###### 2 : 특정 문서에서 특정단어가 많이 등장하고 그 단어가 다른 문서에서 적게 등장할 때, 그 단어를 특정 문서의 핵심어로 간주\n",
    "###### 3 : 어휘 빈도-문서 역빈도는 어휘 빈도와 역문서 빈도를 곱해 계산 가능\n",
    "###### 4 : 어휘 빈도는 특정 문서에서 특정 단어가 많이 등장하는것을 의미\n",
    "###### 5 : 역문서 빈도는 다른 문서에서등장하지 않는 단어 빈도를 의미 \n",
    "###### 6 : 어휘 비도 - 문서 역빈도는 다음과 같이 표현 Wxy=tFx,y*log(N/dFx)\n",
    "\n",
    "###### 7 : tf-idf 를 편리하게 계산하기 위해 scikit-learn의 tfidvectorizer를 이용\n",
    "###### 8 : 앞서 계산한 단어 빈도 수를 입력하여 tf-idf로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66f919b-a3c5-44c7-a957-1fe7f9c6ec15",
   "metadata": {},
   "outputs": [],
   "source": [
    "## sklearn.feature_extraction.text import TfidVectorizer를 사용하기 위해서는 sklearn의 버전이 0.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "70063dad-161f-463e-a109-73bd3af36c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "2e5c8001-e85f-4d82-851a-5e9148f29d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.311383   0.311383   0.         0.         0.62276601 0.4736296\n",
      "  0.         0.311383   0.311383   0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.52753275\n",
      "  0.34682109 0.         0.         0.69364217 0.34682109]\n",
      " [0.         0.         0.70710678 0.70710678 0.         0.\n",
      "  0.         0.         0.         0.         0.        ]]\n",
      "{'think': 7, 'like': 4, 'man': 5, 'action': 1, 'act': 0, 'thought': 8, 'try': 9, 'success': 6, 'value': 10, 'liberty': 3, 'death': 2}\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(stop_words=\"english\").fit(corpus)\n",
    "print(tfidf.transform(corpus).toarray())\n",
    "print(tfidf.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c246f0c3-d5a8-4d9a-a046-8fd7b6011736",
   "metadata": {},
   "source": [
    "###### 보기 편하게 데이터 프레임으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e5030f40-587e-408d-820c-8c2b87b3f913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>death</th>\n",
       "      <th>liberty</th>\n",
       "      <th>like</th>\n",
       "      <th>man</th>\n",
       "      <th>success</th>\n",
       "      <th>think</th>\n",
       "      <th>thought</th>\n",
       "      <th>try</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.311383</td>\n",
       "      <td>0.311383</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.622766</td>\n",
       "      <td>0.473630</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.311383</td>\n",
       "      <td>0.311383</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.527533</td>\n",
       "      <td>0.346821</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693642</td>\n",
       "      <td>0.346821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        act    action     death   liberty      like       man   success  \\\n",
       "0  0.311383  0.311383  0.000000  0.000000  0.622766  0.473630  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.527533  0.346821   \n",
       "2  0.000000  0.000000  0.707107  0.707107  0.000000  0.000000  0.000000   \n",
       "\n",
       "      think   thought       try     value  \n",
       "0  0.311383  0.311383  0.000000  0.000000  \n",
       "1  0.000000  0.000000  0.693642  0.346821  \n",
       "2  0.000000  0.000000  0.000000  0.000000  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = []\n",
    "for k, v in sorted(vector.vocabulary_.items(), key=lambda item: item[1]):\n",
    "    columns.append(k)\n",
    "\n",
    "pd.DataFrame(tfidf.transform(corpus).toarray(), columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee098db9-1ea0-4b39-b37a-df05e2062b2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mecab",
   "language": "python",
   "name": "mecab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
